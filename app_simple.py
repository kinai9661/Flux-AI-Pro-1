import streamlit as st
from openai import OpenAI
from PIL import Image
import requests
from io import BytesIO
import datetime
import base64
from typing import Dict, List, Tuple, Optional
import time
import random
import uuid
import os
from urllib.parse import urlencode, quote
import gc
from streamlit.errors import StreamlitAPIException

# æ‡‰ç”¨é…ç½®
APP_TITLE = "ğŸ¨ AI åœ–åƒç”Ÿæˆå™¨ (å¤šæ¨¡å‹ç‰ˆ)"
VERSION = "v1.5.0"

# é™åˆ¶é…ç½®
MAX_HISTORY_ITEMS = 20
MAX_FAVORITE_ITEMS = 40
MAX_BATCH_SIZE = 4
REQUEST_TIMEOUT = 120

# åœ–åƒå°ºå¯¸é è¨­
IMAGE_SIZES = {
    "è‡ªå®šç¾©...": "Custom",
    "512x512": "SD æ¨™æº– (1:1)", 
    "768x768": "SD XL æ¨™æº– (1:1)",
    "1024x1024": "æ­£æ–¹å½¢ (1:1)", 
    "1080x1080": "IG è²¼æ–‡ (1:1)",
    "512x768": "SD ç¸±å‘ (2:3)",
    "768x1024": "SDXL ç¸±å‘ (3:4)",
    "1080x1350": "IG ç¸±å‘ (4:5)", 
    "1080x1920": "IG Story (9:16)",
    "768x512": "SD æ©«å‘ (3:2)",
    "1024x768": "SDXL æ©«å‘ (4:3)",
    "1200x630": "FB æ©«å‘ (1.91:1)",
    "1536x640": "è¶…å¯¬æ©«å¹… (2.4:1)",
}

# é¢¨æ ¼é è¨­
STYLE_PRESETS = {
    "ç„¡": "",
    "é›»å½±æ„Ÿ": "cinematic, dramatic lighting, high detail, sharp focus, epic scene",
    "å‹•æ¼«é¢¨": "anime, manga style, vibrant colors, clean line art, studio ghibli", 
    "è³½åšé¾å…‹": "cyberpunk, neon lights, futuristic city, high-tech, Blade Runner",
    "äººåƒæ”å½±": "portrait photography, professional headshot, studio lighting, bokeh",
    "è¡—é ­æ”å½±": "street photography, candid moment, urban setting, natural lighting",
    "é¢¨æ™¯æ”å½±": "landscape photography, golden hour lighting, wide angle view, HDR",
    "å°è±¡æ´¾": "impressionism, soft brushstrokes, natural light, Monet style",
    "è¶…ç¾å¯¦ä¸»ç¾©": "surrealism, dreamlike imagery, Salvador Dali style",
    "æ™®æ™®è—è¡“": "pop art, bold colors, comic book style, Andy Warhol",
    "æ°´å¢¨ç•«": "traditional Chinese ink painting, minimalist zen aesthetic",
    "æ°´å½©ç•«": "watercolor painting, soft transparent washes, delicate colors",
    "3D æ¸²æŸ“": "3D render, octane rendering, photorealistic, volumetric lighting",
    "åƒç´ è—è¡“": "pixel art, 8-bit style, retro gaming aesthetic",
    "è’¸æ±½é¾å…‹": "steampunk aesthetic, Victorian era meets technology",
    "å¥‡å¹»è—è¡“": "fantasy art, magical creatures, epic landscapes",
    "ç§‘å¹»è—è¡“": "science fiction art, futuristic technology, space scenes",
    "ç¾å¼æ¼«ç•«": "American comic book style, bold outlines, dynamic poses",
    "æ—¥å¼æ¼«ç•«": "manga style, detailed line art, expressive characters",
    "é»‘ç™½æ”å½±": "black and white photography, high contrast, dramatic shadows",
    "çŸ¢é‡åœ–": "vector illustration, clean geometric lines, flat design",
    "æ²¹ç•«": "oil painting, thick impasto, rich textures, renaissance style",
    "ç´ æ": "pencil sketch, graphite drawing, crosshatching, detailed line work",
    "åŒ…è±ªæ–¯": "Bauhaus design, geometric minimalism, functional aesthetics",
    "è£é£¾è—è¡“": "art deco style, geometric patterns, luxury aesthetics",
}

# è² å‘æç¤ºè©é è¨­
NEGATIVE_PROMPTS = {
    "åŸºæœ¬": "blurry, low quality, distorted, deformed, ugly, bad anatomy",
    "æ”å½±": "blurry, low resolution, overexposed, underexposed, noise",
    "äººåƒ": "bad anatomy, deformed face, extra limbs, missing fingers",
    "å‹•æ¼«": "realistic, photographic, 3d render, western cartoon",
    "è—è¡“": "photographic, realistic, low quality, commercial",
}

def rerun_app():
    try:
        if hasattr(st, 'rerun'):
            st.rerun()
        elif hasattr(st, 'experimental_rerun'):
            st.experimental_rerun()
        else:
            st.stop()
    except Exception:
        st.stop()

st.set_page_config(
    page_title=APP_TITLE,
    page_icon="ğŸ¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# APIä¾›æ‡‰å•†é…ç½®
API_PROVIDERS = {
    "Pollinations.ai": {
        "name": "Pollinations.ai Studio",
        "base_url_default": "https://image.pollinations.ai",
        "icon": "ğŸŒ¸",
        "hardcoded_models": {
            # FLUX ç³»åˆ—
            "flux-1.1-pro": {"name": "Flux 1.1 Pro", "icon": "ğŸ†", "category": "FLUX"},
            "flux.1-kontext-pro": {"name": "Flux.1 Kontext Pro", "icon": "ğŸ§ ", "category": "FLUX"},
            "flux.1-kontext-max": {"name": "Flux.1 Kontext Max", "icon": "ğŸ‘‘", "category": "FLUX"},
            "flux-dev": {"name": "Flux Dev", "icon": "ğŸ› ï¸", "category": "FLUX"},
            "flux-schnell": {"name": "Flux Schnell", "icon": "âš¡", "category": "FLUX"},
            "flux-realism": {"name": "Flux Realism", "icon": "ğŸ“·", "category": "FLUX"},
            # Stable Diffusion ç³»åˆ—
            "stable-diffusion-3.5-large": {"name": "SD 3.5 Large", "icon": "ğŸ¯", "category": "Stable Diffusion"},
            "stable-diffusion-3.5-medium": {"name": "SD 3.5 Medium", "icon": "âš–ï¸", "category": "Stable Diffusion"},
            "stable-diffusion-xl": {"name": "SDXL 1.0", "icon": "ğŸ’", "category": "Stable Diffusion"},
            "stable-diffusion-xl-turbo": {"name": "SDXL Turbo", "icon": "ğŸš€", "category": "Stable Diffusion"},
            "stable-diffusion-2.1": {"name": "SD 2.1", "icon": "ğŸ”„", "category": "Stable Diffusion"},
            "stable-diffusion-1.5": {"name": "SD 1.5", "icon": "ğŸ”°", "category": "Stable Diffusion"},
            # å°ˆæ¥­æ¨¡å‹
            "midjourney": {"name": "Midjourney", "icon": "ğŸ­", "category": "Professional"},
            "dalle-3": {"name": "DALL-E 3", "icon": "ğŸ¤–", "category": "Professional"},
            "playground-v2.5": {"name": "Playground v2.5", "icon": "ğŸª", "category": "Professional"},
            # ç¤¾å€æ¨¡å‹
            "dreamshaper": {"name": "DreamShaper", "icon": "ğŸ’«", "category": "Community"},
            "realistic-vision": {"name": "Realistic Vision", "icon": "ğŸ‘ï¸", "category": "Community"},
            "deliberate": {"name": "Deliberate", "icon": "ğŸ¨", "category": "Community"},
            "anything-v5": {"name": "Anything v5", "icon": "ğŸŒŸ", "category": "Anime"},
            "waifu-diffusion": {"name": "Waifu Diffusion", "icon": "ğŸ‘©â€ğŸ¨", "category": "Anime"},
            "openjourney": {"name": "OpenJourney", "icon": "ğŸ—ºï¸", "category": "Community"},
            # é¢¨æ ¼æ¨¡å‹
            "analog-diffusion": {"name": "Analog Film", "icon": "ğŸ“¸", "category": "Style"},
            "synthwave-diffusion": {"name": "Synthwave", "icon": "ğŸŒ†", "category": "Style"},
            "cyberpunk-anime": {"name": "Cyberpunk Anime", "icon": "ğŸ¤–", "category": "Style"},
            "pixel-art-xl": {"name": "Pixel Art XL", "icon": "ğŸ®", "category": "Style"},
        }
    },
    "NavyAI": {
        "name": "NavyAI",
        "base_url_default": "https://api.navy/v1",
        "icon": "âš“",
        "hardcoded_models": {
            "flux-pro": {"name": "Flux Pro", "icon": "ğŸ†", "category": "FLUX"},
            "flux-schnell": {"name": "Flux Schnell", "icon": "âš¡", "category": "FLUX"},
            "stable-diffusion-xl": {"name": "SDXL", "icon": "ğŸ’", "category": "Stable Diffusion"},
            "midjourney-v6": {"name": "Midjourney v6", "icon": "ğŸ­", "category": "Professional"},
        }
    },
    "Hugging Face": {
        "name": "Hugging Face Inference",
        "base_url_default": "https://api-inference.huggingface.co",
        "icon": "ğŸ¤—",
        "hardcoded_models": {
            "stable-diffusion-v1-5": {"name": "SD 1.5 (HF)", "icon": "ğŸ”°", "category": "Stable Diffusion"},
            "stable-diffusion-xl-base-1.0": {"name": "SDXL Base (HF)", "icon": "ğŸ’", "category": "Stable Diffusion"},
            "flux-1-dev": {"name": "Flux.1 Dev (HF)", "icon": "ğŸ› ï¸", "category": "FLUX"},
        }
    },
    "OpenAI Compatible": {
        "name": "OpenAI å…¼å®¹ API",
        "base_url_default": "https://api.openai.com/v1",
        "icon": "ğŸ¤–",
        "hardcoded_models": {
            "dall-e-3": {"name": "DALL-E 3", "icon": "ğŸ¤–", "category": "OpenAI"},
            "dall-e-2": {"name": "DALL-E 2", "icon": "ğŸ”„", "category": "OpenAI"},
        }
    }
}

def init_session_state():
    if 'api_profiles' not in st.session_state:
        try:
            base_profiles = st.secrets.get("api_profiles", {})
        except:
            base_profiles = {}
        
        default_profiles = {
            "é è¨­ Pollinations": {
                'provider': 'Pollinations.ai',
                'api_key': '',
                'base_url': 'https://image.pollinations.ai',
                'validated': True,
                'pollinations_auth_mode': 'å…è²»',
                'pollinations_token': '',
                'pollinations_referrer': ''
            }
        }
        
        st.session_state.api_profiles = base_profiles.copy() if base_profiles else default_profiles
    
    if ('active_profile_name' not in st.session_state or 
        st.session_state.active_profile_name not in st.session_state.api_profiles):
        st.session_state.active_profile_name = (
            list(st.session_state.api_profiles.keys())[0] 
            if st.session_state.api_profiles else ""
        )
    
    defaults = {
        'generation_history': [],
        'favorite_images': [],
        'discovered_models': {},
        'selected_model': None,
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

def get_active_config():
    return st.session_state.api_profiles.get(st.session_state.active_profile_name, {})

def validate_api_key(api_key: str, base_url: str, provider: str) -> Tuple[bool, str]:
    try:
        if provider == "Pollinations.ai":
            return True, "Pollinations.ai ç„¡éœ€é©—è­‰"
        
        elif provider == "Hugging Face":
            if not api_key:
                return False, "Hugging Face éœ€è¦ API Token"
            headers = {"Authorization": f"Bearer {api_key}"}
            response = requests.get(f"{base_url}/models", headers=headers, timeout=10)
            if response.status_code == 200:
                return True, "Hugging Face API Token é©—è­‰æˆåŠŸ"
            else:
                return False, f"Hugging Face API é©—è­‰å¤±æ•—: {response.status_code}"
        
        else:
            client = OpenAI(api_key=api_key, base_url=base_url)
            client.models.list()
            return True, "API å¯†é‘°é©—è­‰æˆåŠŸ"
            
    except Exception as e:
        return False, f"API é©—è­‰å¤±æ•—: {str(e)[:100]}"

def get_models_by_category(models: Dict[str, Dict]) -> Dict[str, Dict[str, Dict]]:
    categorized = {}
    for model_id, model_info in models.items():
        category = model_info.get('category', 'Other')
        if category not in categorized:
            categorized[category] = {}
        categorized[category][model_id] = model_info
    
    priority_order = ["FLUX", "Stable Diffusion", "Professional", "Anime", "Style", "Community", "Other"]
    sorted_categorized = {}
    
    for category in priority_order:
        if category in categorized:
            sorted_categorized[category] = categorized[category]
    
    for category, models in categorized.items():
        if category not in sorted_categorized:
            sorted_categorized[category] = models
    
    return sorted_categorized

def merge_models() -> Dict[str, Dict]:
    provider = get_active_config().get('provider')
    discovered = st.session_state.get('discovered_models', {})
    
    if provider in API_PROVIDERS:
        hardcoded = API_PROVIDERS[provider].get('hardcoded_models', {})
        merged = {**hardcoded, **discovered}
    else:
        merged = discovered
    
    return merged

def generate_images_with_retry(client, **params) -> Tuple[bool, any]:
    provider = get_active_config().get('provider')
    n_images = params.get("n", 1)
    
    if provider == "Pollinations.ai":
        return generate_pollinations_images(params, n_images)
    elif provider == "Hugging Face":
        return generate_huggingface_images(params, n_images)
    else:
        return generate_openai_compatible_images(client, params, n_images)

def generate_pollinations_images(params: Dict, n_images: int) -> Tuple[bool, any]:
    generated_images = []
    cfg = get_active_config()
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i in range(n_images):
        try:
            status_text.text(f"æ­£åœ¨ç”Ÿæˆç¬¬ {i+1}/{n_images} å¼µåœ–ç‰‡...")
            progress_bar.progress(i / n_images)
            
            current_params = params.copy()
            current_params["seed"] = random.randint(0, 2**32 - 1)
            
            prompt = current_params.get("prompt", "")
            if neg_prompt := current_params.get("negative_prompt"):
                prompt += f" --no {neg_prompt}"
            
            width, height = str(current_params.get("size", "1024x1024")).split('x')
            
            api_params = {}
            for key, value in {
                "model": current_params.get("model"),
                "width": width,
                "height": height,
                "seed": current_params.get("seed"),
                "nologo": current_params.get("nologo"),
                "private": current_params.get("private"),
                "enhance": current_params.get("enhance"),
                "safe": current_params.get("safe")
            }.items():
                if value is not None:
                    api_params[key] = value
            
            headers = {}
            auth_mode = cfg.get('pollinations_auth_mode', 'å…è²»')
            
            if auth_mode == 'ä»¤ç‰Œ' and cfg.get('pollinations_token'):
                headers['Authorization'] = f"Bearer {cfg['pollinations_token']}"
            elif auth_mode == 'åŸŸå' and cfg.get('pollinations_referrer'):
                headers['Referer'] = cfg['pollinations_referrer']
            
            url = f"{cfg['base_url']}/prompt/{quote(prompt)}?{urlencode(api_params)}"
            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            
            if response.ok:
                b64_json = base64.b64encode(response.content).decode()
                image_obj = type('Image', (object,), {'b64_json': b64_json})
                generated_images.append(image_obj)
            else:
                st.warning(f"ç¬¬ {i+1} å¼µåœ–ç‰‡ç”Ÿæˆå¤±æ•—: HTTP {response.status_code}")
                
        except Exception as e:
            st.warning(f"ç¬¬ {i+1} å¼µåœ–ç‰‡ç”ŸæˆéŒ¯èª¤: {str(e)[:100]}")
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"å®Œæˆç”Ÿæˆ {len(generated_images)}/{n_images} å¼µåœ–ç‰‡")
    time.sleep(1)
    progress_bar.empty()
    status_text.empty()
    
    if generated_images:
        response_obj = type('Response', (object,), {'data': generated_images})
        return True, response_obj
    else:
        return False, "æ‰€æœ‰åœ–ç‰‡ç”Ÿæˆå‡å¤±æ•—"

def generate_huggingface_images(params: Dict, n_images: int) -> Tuple[bool, any]:
    generated_images = []
    cfg = get_active_config()
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i in range(n_images):
        try:
            status_text.text(f"æ­£åœ¨é€šéHFç”Ÿæˆç¬¬ {i+1}/{n_images} å¼µåœ–ç‰‡...")
            progress_bar.progress(i / n_images)
            
            headers = {"Authorization": f"Bearer {cfg['api_key']}"}
            model = params.get("model")
            prompt = params.get("prompt", "")
            
            payload = {
                "inputs": prompt,
                "parameters": {
                    "negative_prompt": params.get("negative_prompt", ""),
                    "num_inference_steps": 25,
                    "guidance_scale": 7.5,
                }
            }
            
            url = f"{cfg['base_url']}/models/{model}"
            response = requests.post(url, headers=headers, json=payload, timeout=REQUEST_TIMEOUT)
            
            if response.ok:
                b64_json = base64.b64encode(response.content).decode()
                image_obj = type('Image', (object,), {'b64_json': b64_json})
                generated_images.append(image_obj)
            else:
                st.warning(f"ç¬¬ {i+1} å¼µåœ–ç‰‡ç”Ÿæˆå¤±æ•—: HTTP {response.status_code}")
                
        except Exception as e:
            st.warning(f"ç¬¬ {i+1} å¼µåœ–ç‰‡ç”ŸæˆéŒ¯èª¤: {str(e)[:100]}")
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"å®Œæˆç”Ÿæˆ {len(generated_images)}/{n_images} å¼µåœ–ç‰‡")
    time.sleep(1)
    progress_bar.empty()
    status_text.empty()
    
    if generated_images:
        response_obj = type('Response', (object,), {'data': generated_images})
        return True, response_obj
    else:
        return False, "æ‰€æœ‰åœ–ç‰‡ç”Ÿæˆå‡å¤±æ•—"

def generate_openai_compatible_images(client, params: Dict, n_images: int) -> Tuple[bool, any]:
    try:
        sdk_params = {
            "model": params.get("model"),
            "prompt": params.get("prompt"),
            "size": str(params.get("size")),
            "n": n_images,
            "response_format": "b64_json"
        }
        
        if params.get("negative_prompt"):
            sdk_params["negative_prompt"] = params.get("negative_prompt")
        
        sdk_params = {k: v for k, v in sdk_params.items() if v is not None and v != ""}
        return True, client.images.generate(**sdk_params)
    except Exception as e:
        return False, str(e)[:200]

def add_to_history(prompt: str, negative_prompt: str, model: str, images: List[str], metadata: Dict):
    history = st.session_state.generation_history
    new_entry = {
        "id": str(uuid.uuid4()),
        "timestamp": datetime.datetime.now(),
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "model": model,
        "images": images,
        "metadata": metadata
    }
    history.insert(0, new_entry)
    st.session_state.generation_history = history[:MAX_HISTORY_ITEMS]

def display_image_with_actions(b64_json: str, image_id: str, history_item: Dict):
    try:
        img_data = base64.b64decode(b64_json)
        img = Image.open(BytesIO(img_data))
        st.image(img, use_container_width=True)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰",
                img_data,
                f"ai_generated_{image_id}.png",
                "image/png",
                key=f"dl_{image_id}",
                use_container_width=True
            )
        
        with col2:
            is_fav = any(fav['id'] == image_id for fav in st.session_state.favorite_images)
            if st.button(
                "â­" if is_fav else "â˜†",
                key=f"fav_{image_id}",
                use_container_width=True,
                help="æ”¶è—/å–æ¶ˆæ”¶è—"
            ):
                if is_fav:
                    st.session_state.favorite_images = [
                        f for f in st.session_state.favorite_images
                        if f['id'] != image_id
                    ]
                else:
                    if len(st.session_state.favorite_images) < MAX_FAVORITE_ITEMS:
                        st.session_state.favorite_images.append({
                            "id": image_id,
                            "image_b64": b64_json,
                            "timestamp": datetime.datetime.now(),
                            "history_item": history_item
                        })
                    else:
                        st.warning(f"æ”¶è—å·²é”ä¸Šé™ ({MAX_FAVORITE_ITEMS})")
                rerun_app()
        
        with col3:
            if st.button(
                "ğŸ¨ è®Šé«”",
                key=f"vary_{image_id}",
                use_container_width=True,
                help="ä½¿ç”¨æ­¤æç¤ºç”Ÿæˆè®Šé«”"
            ):
                st.session_state.update({
                    'vary_prompt': history_item['prompt'],
                    'vary_negative_prompt': history_item.get('negative_prompt', ''),
                    'vary_model': history_item['model']
                })
                rerun_app()
                
    except Exception as e:
        st.error(f"åœ–åƒé¡¯ç¤ºéŒ¯èª¤: {str(e)[:100]}")

def init_api_client():
    cfg = get_active_config()
    if (cfg and cfg.get('api_key') and cfg.get('provider') not in ["Pollinations.ai", "Hugging Face"]):
        try:
            return OpenAI(api_key=cfg['api_key'], base_url=cfg['base_url'])
        except Exception:
            return None
    return None

def show_model_selector(all_models: Dict[str, Dict]) -> Optional[str]:
    if not all_models:
        st.warning("âš ï¸ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ã€‚è«‹åœ¨å´é‚Šæ¬„é…ç½®APIã€‚")
        return None
    
    categorized_models = get_models_by_category(all_models)
    current_selection = st.session_state.get('selected_model')
    
    if current_selection not in all_models:
        current_selection = list(all_models.keys())[0]
        st.session_state.selected_model = current_selection
    
    if current_selection:
        model_info = all_models[current_selection]
        st.success(
            f"ğŸ¯ ç•¶å‰æ¨¡å‹: {model_info.get('icon', 'ğŸ¤–')} "
            f"{model_info.get('name', current_selection)}"
        )
    
    st.markdown("---")
    total_models = len(all_models)
    categories_count = len(categorized_models)
    st.caption(f"ğŸ“Š å¯ç”¨æ¨¡å‹: **{total_models}** å€‹ï¼Œåˆ†ç‚º **{categories_count}** å€‹é¡åˆ¥")
    
    for category, models in categorized_models.items():
        with st.expander(f"ğŸ“ {category} ({len(models)} å€‹æ¨¡å‹)", expanded=True):
            cols = st.columns(3)
            
            for i, (model_id, model_info) in enumerate(models.items()):
                col = cols[i % 3]
                
                with col:
                    button_text = f"{model_info.get('icon', 'ğŸ¤–')} {model_info.get('name', model_id)}"
                    is_selected = st.session_state.get('selected_model') == model_id
                    
                    if st.button(
                        button_text,
                        key=f"select_model_{model_id}_{category}",
                        use_container_width=True,
                        type="primary" if is_selected else "secondary"
                    ):
                        st.session_state.selected_model = model_id
                        rerun_app()
    
    return current_selection

def main():
    init_session_state()
    client = init_api_client()
    cfg = get_active_config()
    api_configured = cfg and cfg.get('validated', False)
    
    # å´é‚Šæ¬„
    with st.sidebar:
        st.subheader("âš™ï¸ API è¨­ç½®")
        
        if api_configured:
            provider_info = API_PROVIDERS.get(cfg['provider'], {})
            st.success(f"ğŸŸ¢ å·²é€£æ¥: {st.session_state.active_profile_name}")
            st.info(f"{provider_info.get('icon', 'ğŸ¤–')} {provider_info.get('name', cfg['provider'])}")
        else:
            st.warning("âš ï¸ è«‹é…ç½®APIä¾›æ‡‰å•†")
        
        st.markdown("---")
        st.info(f"""
        **ğŸ“Š çµ±è¨ˆä¿¡æ¯**
        - æ­·å²: {len(st.session_state.generation_history)}/{MAX_HISTORY_ITEMS}
        - æ”¶è—: {len(st.session_state.favorite_images)}/{MAX_FAVORITE_ITEMS}
        - æ‰¹æ¬¡ä¸Šé™: {MAX_BATCH_SIZE}
        """)
    
    # ä¸»æ¨™é¡Œ
    st.title(APP_TITLE)
    st.caption(f"æ”¯æ´ FLUXã€Stable Diffusionã€DALL-E åŠæ›´å¤šæ¨¡å‹ | {VERSION}")
    
    # ä¸»ç•Œé¢
    tab1, tab2, tab3 = st.tabs([
        "ğŸš€ ç”Ÿæˆåœ–åƒ",
        f"ğŸ“š æ­·å² ({len(st.session_state.generation_history)})",
        f"â­ æ”¶è— ({len(st.session_state.favorite_images)})"
    ])
    
    with tab1:
        if not api_configured:
            st.warning("âš ï¸ è«‹åœ¨å´é‚Šæ¬„é…ç½®ä¸¦é©—è­‰APIä¾›æ‡‰å•†")
        else:
            all_models = merge_models()
            selected_model = show_model_selector(all_models)
            
            if selected_model:
                st.markdown("---")
                
                # ç”Ÿæˆåƒæ•¸
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    selected_style = st.selectbox(
                        "ğŸ¨ é¢¨æ ¼é è¨­",
                        list(STYLE_PRESETS.keys())
                    )
                    
                    prompt_val = st.text_area(
                        "âœï¸ æç¤ºè©",
                        value=st.session_state.pop('vary_prompt', ''),
                        height=120,
                        placeholder="æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„åœ–åƒ..."
                    )
                    
                    neg_preset = st.selectbox(
                        "ğŸš« è² å‘æç¤ºè©é è¨­",
                        list(NEGATIVE_PROMPTS.keys())
                    )
                    
                    negative_prompt_val = st.text_area(
                        "ğŸš« è² å‘æç¤ºè©",
                        value=st.session_state.pop('vary_negative_prompt', '') or NEGATIVE_PROMPTS.get(neg_preset, ""),
                        height=80,
                        placeholder="ä¸æƒ³è¦çš„å…§å®¹..."
                    )
                
                with col2:
                    n_images = st.slider(
                        "ğŸ–¼ï¸ ç”Ÿæˆæ•¸é‡",
                        1, MAX_BATCH_SIZE, 1
                    )
                    
                    size_preset = st.selectbox(
                        "ğŸ“ åœ–åƒå°ºå¯¸",
                        options=list(IMAGE_SIZES.keys()),
                        format_func=lambda x: IMAGE_SIZES[x]
                    )
                    
                    if size_preset == "è‡ªå®šç¾©...":
                        col_w, col_h = st.columns(2)
                        with col_w:
                            width = st.slider("å¯¬åº¦", 256, 2048, 1024, 64)
                        with col_h:
                            height = st.slider("é«˜åº¦", 256, 2048, 1024, 64)
                        final_size_str = f"{width}x{height}"
                    else:
                        final_size_str = size_preset
                
                # é«˜ç´šé¸é …
                advanced_options = {}
                if cfg.get('provider') == "Pollinations.ai":
                    with st.expander("ğŸŒ¸ Pollinations.ai é€²éšé¸é …"):
                        col1, col2 = st.columns(2)
                        with col1:
                            advanced_options['enhance'] = st.checkbox("âœ¨ å¢å¼·æç¤ºè©", True)
                            advanced_options['private'] = st.checkbox("ğŸ”’ ç§å¯†æ¨¡å¼", True)
                        with col2:
                            advanced_options['nologo'] = st.checkbox("ğŸš« ç§»é™¤æ¨™èªŒ", True)
                            advanced_options['safe'] = st.checkbox("ğŸ›¡ï¸ å®‰å…¨æ¨¡å¼", False)
                
                # ç”ŸæˆæŒ‰éˆ•
                if st.button(
                    "ğŸš€ ç”Ÿæˆåœ–åƒ",
                    type="primary",
                    use_container_width=True,
                    disabled=not prompt_val.strip()
                ):
                    final_prompt = prompt_val
                    if selected_style != "ç„¡" and STYLE_PRESETS[selected_style]:
                        final_prompt = f"{final_prompt}, {STYLE_PRESETS[selected_style]}"
                    
                    params = {
                        "model": selected_model,
                        "prompt": final_prompt,
                        "negative_prompt": negative_prompt_val,
                        "size": final_size_str,
                        "n": n_images,
                        **advanced_options
                    }
                    
                    model_name = all_models[selected_model]['name']
                    with st.spinner(f"ğŸ¨ æ­£åœ¨ä½¿ç”¨ {model_name} ç”Ÿæˆ {n_images} å¼µåœ–åƒ..."):
                        success, result = generate_images_with_retry(client, **params)
                    
                    if success and hasattr(result, 'data') and result.data:
                        img_b64s = [img.b64_json for img in result.data]
                        
                        add_to_history(
                            prompt_val,
                            negative_prompt_val,
                            selected_model,
                            img_b64s,
                            {
                                "size": final_size_str,
                                "provider": cfg['provider'],
                                "style": selected_style,
                                "n": n_images,
                                "model_name": model_name
                            }
                        )
                        
                        st.success(f"âœ¨ æˆåŠŸç”Ÿæˆ {len(img_b64s)} å¼µåœ–åƒï¼")
                        
                        if len(img_b64s) == 1:
                            display_image_with_actions(
                                img_b64s[0],
                                f"{st.session_state.generation_history[0]['id']}_0",
                                st.session_state.generation_history[0]
                            )
                        else:
                            cols = st.columns(2)
                            for i, b64_json in enumerate(img_b64s):
                                with cols[i % 2]:
                                    display_image_with_actions(
                                        b64_json,
                                        f"{st.session_state.generation_history[0]['id']}_{i}",
                                        st.session_state.generation_history[0]
                                    )
                        
                        gc.collect()
                    else:
                        st.error(f"âŒ ç”Ÿæˆå¤±æ•—: {result}")
    
    with tab2:
        if not st.session_state.generation_history:
            st.info("ğŸ“­ é‚„æ²’æœ‰ç”Ÿæˆæ­·å²ã€‚")
        else:
            for item in st.session_state.generation_history:
                timestamp_str = item['timestamp'].strftime('%m-%d %H:%M')
                all_models = merge_models()
                model_info = all_models.get(item['model'], {})
                model_name = model_info.get('name', item['model'])
                
                with st.expander(f"ğŸ¨ {item['prompt'][:60]}... | {model_name} | {timestamp_str}"):
                    st.markdown(f"**âœï¸ æç¤ºè©:** {item['prompt']}")
                    if item.get('negative_prompt'):
                        st.markdown(f"**ğŸš« è² å‘æç¤ºè©:** {item['negative_prompt']}")
                    
                    if len(item['images']) == 1:
                        display_image_with_actions(
                            item['images'][0],
                            f"hist_{item['id']}_0",
                            item
                        )
                    else:
                        cols = st.columns(2)
                        for i, b64_json in enumerate(item['images']):
                            with cols[i % 2]:
                                display_image_with_actions(
                                    b64_json,
                                    f"hist_{item['id']}_{i}",
                                    item
                                )
    
    with tab3:
        if not st.session_state.favorite_images:
            st.info("â­ é‚„æ²’æœ‰æ”¶è—çš„åœ–åƒã€‚")
        else:
            sorted_favorites = sorted(
                st.session_state.favorite_images,
                key=lambda x: x['timestamp'],
                reverse=True
            )
            
            cols = st.columns(3)
            for i, fav in enumerate(sorted_favorites):
                with cols[i % 3]:
                    display_image_with_actions(
                        fav['image_b64'],
                        fav['id'],
                        fav.get('history_item', {})
                    )
                    
                    fav_time = fav['timestamp'].strftime('%m-%d %H:%M')
                    st.caption(f"â­ æ”¶è—æ–¼: {fav_time}")
    
    # é è…³
    st.markdown("---")
    st.markdown(f"""
    <div style="text-align: center; color: #888; margin-top: 2rem;">
        <small>
            ğŸ¨ <strong>AI åœ–åƒç”Ÿæˆå™¨ {VERSION}</strong> | 
            æ”¯æŒå¤šæ¨¡å‹å’Œå¤šä¾›æ‡‰å•† | 
            è®“å‰µæ„ç„¡é™å»¶ä¼¸ ğŸ¨
        </small>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
